{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk Recommender - Pycon 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 32 tuotorials, 12 sponsor workshops, 16 talks at the education summit, and 95 talks at the main conference - Pycon has a lot to offer. Reading through all the talk descriptions and filtering out the ones that you should go to is a tedious process. But not anymore.\n",
    "\n",
    "## Introducing TalkRecommender\n",
    "Talk recommender is a recommendation system that recommends talks from this year's Pycon based on the ones that you went to last year.  This way you don't waste any time preparing a schedule and get to see the talks that matter the most to you! \n",
    "\n",
    "As shown in the demo, the users are asked to label previous year's talks into two categories - the one that they went to in person, and the ones they watched later online. Talk Recommender uses those labels to predict talks from this year that will be interesing to them. \n",
    "\n",
    "We will be using [`pandas`](https://pandas.pydata.org/) abd [`scikit-learn`](http://scikit-learn.org/) to build and the model.\n",
    "\n",
    "*Remember to click on Save and Checkpoint from the File menu to save changes you made to the notebook* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A: Load the data\n",
    "The data directory contains the snapshot of one such user's labeling - lets load that up and start with our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('data/talks.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief description of the interesting fields.\n",
    "\n",
    "variable | description  \n",
    "------|------|\n",
    "`title`|Title of the talk\n",
    "`description`|Description of the talk\n",
    "`year`|Is it a `2017` talk or `2018`  \n",
    "`label`|`1` indicates the user preferred seeing the talk in person,<br> `0` indicates they would schedule it for later.\n",
    "\n",
    "Note all 2018 talks are set to 1. However they are only placeholders, and are not used in training the model. We will  use only 2017 data for training.\n",
    "\n",
    "Lets start by selecting the 2017 talk descriptions that were labeled by the user for watching in person.\n",
    "\n",
    "```python\n",
    "df[(df.year==2017) & (df.label==1)]['description']\n",
    "```\n",
    "\n",
    "Print the description of the talks that the user preferred watching in person. How many such talks are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B: Feature Extraction\n",
    "In this step we build the feature set by tokenization, counting and normalization of the bi-grams from the text descriptions of the talk. You can find more information on text feature extraction [here](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) and TfidfVectorizer [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2017 talks will be used for training and the 2018 talks will we used for predicting. Set the values of `year_labeled` and `year_predict` to appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_labeled=\n",
    "year_predict=\n",
    "vectorized_text_labeled = vectorizer.fit_transform(df[df.year==year_labeled]['description'])\n",
    "vectorized_text_predict = vectorizer.transform(df[df.year==year_predict]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C: Split into Training and Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split our data into training set and testing set. This allows us to do cross validation and avoid overfitting. Use the `train_test_split` method from `sklearn.model_selection` to split the `vectorized_text_labeled` into training and testing set with the test size as one third of the size (0.3) of the labeled.\n",
    "\n",
    "[Here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) is the documentation for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels = df[df.year == 2017]['label']\n",
    "test_size=\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_text_labeled, labels, test_size=test_size, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise D: Train the model\n",
    "Finally we get to the stage for training the model. We are going to use a linear [support vector classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) and check its accuracy by using the `classification_report` function. Note that we have not done any parameter tuning yet, so your model might not give you the best results. \n",
    "\n",
    "\n",
    "[Here](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html) is some information for using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) for doing exhaustive search over specified parameter values of an estimator. _However, this is purely for reference and not needed for this exercise._\n",
    "\n",
    "Print out the `report` to see how well your model has been trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      1.00      0.83        20\n",
      "        1.0       1.00      0.11      0.20         9\n",
      "\n",
      "avg / total       0.80      0.72      0.64        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise E: Make Predictions\n",
    "Use the model to predict which 2018 talks the user should go to. \n",
    "\n",
    "Using the `predicted_talk_indexes` print out the talk id, description, presenters, title and location and talk date.\n",
    "How many talks should the user go to according to your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_talks_vector = classifier.predict(vectorized_text_predict)\n",
    "df_2018 = df[df.year==2018]\n",
    "\n",
    "# Offset the rows by 2017 talks\n",
    "predicted_talk_indexes = predicted_talks_vector.nonzero()[0] + len(df[df.year==2017])\n",
    "# your solution goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise F: Expose it as a service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have pieces of the code ready, copy them together into the `model.py` file located in this folder, and rebuild your docker image. Copy the code from the above cells into the body of the `prediction` function.\n",
    "\n",
    "Lets rebuild the docker image and start an new container following the comments.\n",
    "\n",
    "In the following steps you will leave the jupyter notebook, and stop the container serving it. So save any changes you have done till this point.\n",
    "\n",
    "```\n",
    "docker stop <container_name>\n",
    "docker build -t recommender .\n",
    "docker run -p 8888:8888 -p 9000:9000 -v $(pwd):/app recommender\n",
    "```\n",
    "where `<container_name>` is the name of the container serving this jupyter notebook.\n",
    "\n",
    "The `api.py` file in this directory is a flask app that makes call to the `model.py` module and exposes the model built in the previous steps as a service. In order to start the flask server, open a new terminal and run the following command.\n",
    "\n",
    "```\n",
    "docker exec $(docker ps -ql) python api.py\n",
    "```\n",
    "Where `docker ps -ql` gets numeric id of the latest container id.\n",
    "\n",
    "Finally go to http://0.0.0.0:9000/predict to see the talks that were recommended for this user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise G: Pickle the model\n",
    "\n",
    "Finally we do not have to retrain our model anytime we have to make predictions. In most real life data science applications, the training phase is a time consuming proecss. We would seaprately train and serialize the model which is then exposed through the api to make the predictions. The `predict_api` directory of the TalkVoter app shows an approach where we wrap the model and seaprate out only calls to the prediction api to use the trained model instead of reprocessing any time there is a call to the api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "with open('talk_recommender.pkl', 'wb') as f:\n",
    "    joblib.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create the pickle file in your directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `joblib.load` function to read the `classifier` back from the `talk_recommender.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
